{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# í•œêµ­ì–´ ë°©ì–¸ íƒì§€ ëª¨ë¸ í•™ìŠµ\n",
        "\n",
        "ì´ ë…¸íŠ¸ë¶ì€ Wav2Vec2 ëª¨ë¸ì„ í•œêµ­ì–´ ë°©ì–¸ ë¶„ë¥˜ì— ë§ê²Œ íŒŒì¸íŠœë‹í•©ë‹ˆë‹¤.\n",
        "\n",
        "## í•„ìš”í•œ ì‚¬ì „ ì¤€ë¹„\n",
        "\n",
        "### 1. ë°ì´í„°ì…‹ êµ¬ì¡°\n",
        "SSDì— ì €ì¥ëœ ë°ì´í„°ë¥¼ ë‹¤ìŒê³¼ ê°™ì€ êµ¬ì¡°ë¡œ ì¤€ë¹„í•´ì£¼ì„¸ìš”:\n",
        "```\n",
        "/path/to/your/dialect_dataset/\n",
        "â”œâ”€â”€ standard/          # í‘œì¤€ì–´ (ì„œìš¸/ìˆ˜ë„ê¶Œ)\n",
        "â”‚   â”œâ”€â”€ audio_001.wav\n",
        "â”‚   â”œâ”€â”€ audio_002.wav\n",
        "â”‚   â””â”€â”€ ...\n",
        "â”œâ”€â”€ gyeongsang/        # ê²½ìƒë„ ë°©ì–¸\n",
        "â”‚   â”œâ”€â”€ audio_001.wav\n",
        "â”‚   â””â”€â”€ ...\n",
        "â”œâ”€â”€ jeolla/            # ì „ë¼ë„ ë°©ì–¸\n",
        "â”‚   â”œâ”€â”€ audio_001.wav\n",
        "â”‚   â””â”€â”€ ...\n",
        "â”œâ”€â”€ chungcheong/       # ì¶©ì²­ë„ ë°©ì–¸\n",
        "â”‚   â”œâ”€â”€ audio_001.wav\n",
        "â”‚   â””â”€â”€ ...\n",
        "â””â”€â”€ gangwon/           # ê°•ì›ë„ ë°©ì–¸ (ì„ íƒì‚¬í•­)\n",
        "    â”œâ”€â”€ audio_001.wav\n",
        "    â””â”€â”€ ...\n",
        "```\n",
        "\n",
        "### 2. ì˜¤ë””ì˜¤ íŒŒì¼ ìš”êµ¬ì‚¬í•­\n",
        "- í˜•ì‹: WAV ë˜ëŠ” MP3\n",
        "- ìƒ˜í”Œë§ ë ˆì´íŠ¸: ê°€ê¸‰ì  16kHz (ìë™ìœ¼ë¡œ ë¦¬ìƒ˜í”Œë§ë¨)\n",
        "- ê¸¸ì´: 2ì´ˆ ~ 30ì´ˆ ê¶Œì¥\n",
        "- ê° ë°©ì–¸ë‹¹ ìµœì†Œ 100ê°œ ì´ìƒì˜ ìƒ˜í”Œ ê¶Œì¥\n",
        "\n",
        "### 3. í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬\n",
        "ì´ ë…¸íŠ¸ë¶ ì‹¤í–‰ ì „ì— ë‹¤ìŒ ëª…ë ¹ì–´ë¡œ í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì„¤ì¹˜í•˜ì„¸ìš”:\n",
        "```bash\n",
        "pip install transformers datasets torch torchaudio accelerate librosa soundfile scikit-learn evaluate jiwer\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: í™˜ê²½ ì„¤ì • ë° ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "from typing import Dict, List, Tuple\n",
        "\n",
        "# Hugging Face libraries\n",
        "from datasets import load_dataset, Audio, DatasetDict\n",
        "from transformers import (\n",
        "    Wav2Vec2Processor,\n",
        "    Wav2Vec2ForSequenceClassification,\n",
        "    TrainingArguments,\n",
        "    Trainer,\n",
        ")\n",
        "from transformers import EarlyStoppingCallback\n",
        "\n",
        "# Evaluation\n",
        "import evaluate\n",
        "\n",
        "# Audio processing\n",
        "import librosa\n",
        "import soundfile as sf\n",
        "\n",
        "# ML utilities\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import torch\n",
        "\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: ì„¤ì • (ì—¬ê¸°ë¥¼ ìˆ˜ì •í•˜ì„¸ìš”!)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================\n",
        "# ğŸ”§ ì—¬ê¸°ë¥¼ ë³¸ì¸ì˜ í™˜ê²½ì— ë§ê²Œ ìˆ˜ì •í•˜ì„¸ìš”!\n",
        "# ============================================\n",
        "\n",
        "# ë°ì´í„°ì…‹ ê²½ë¡œ (SSDì— ì €ì¥ëœ ë°©ì–¸ ë°ì´í„° ê²½ë¡œ)\n",
        "DATASET_PATH = \"/path/to/your/ssd/dialect_dataset\"  # ì—¬ê¸°ë¥¼ ìˆ˜ì •!\n",
        "\n",
        "# ëª¨ë¸ ì €ì¥ ê²½ë¡œ\n",
        "OUTPUT_DIR = \"../models/dialect_classifier\"\n",
        "\n",
        "# ì‚¬ì „ í•™ìŠµëœ í•œêµ­ì–´ Wav2Vec2 ëª¨ë¸\n",
        "# ì˜µì…˜ 1: kresnik/wav2vec2-large-xlsr-korean (ì¶”ì²œ)\n",
        "# ì˜µì…˜ 2: facebook/wav2vec2-large-xlsr-53 (ë‹¤êµ­ì–´)\n",
        "BASE_MODEL = \"kresnik/wav2vec2-large-xlsr-korean\"\n",
        "\n",
        "# í•™ìŠµ ì„¤ì •\n",
        "BATCH_SIZE = 8  # GPU ë©”ëª¨ë¦¬ì— ë”°ë¼ ì¡°ì • (8, 16, 32)\n",
        "NUM_EPOCHS = 10\n",
        "LEARNING_RATE = 3e-5\n",
        "EVAL_STEPS = 100  # í‰ê°€ ì£¼ê¸°\n",
        "SAVE_STEPS = 100  # ì²´í¬í¬ì¸íŠ¸ ì €ì¥ ì£¼ê¸°\n",
        "\n",
        "# ì˜¤ë””ì˜¤ ì„¤ì •\n",
        "SAMPLE_RATE = 16000\n",
        "MAX_DURATION = 30  # ì´ˆ ë‹¨ìœ„ (30ì´ˆ ì´ìƒì€ ì˜ë¦¼)\n",
        "\n",
        "# GPU ì‚¬ìš© ì„¤ì •\n",
        "USE_GPU = torch.cuda.is_available()\n",
        "DEVICE = \"cuda\" if USE_GPU else \"cpu\"\n",
        "\n",
        "print(f\"ğŸ“‚ ë°ì´í„°ì…‹ ê²½ë¡œ: {DATASET_PATH}\")\n",
        "print(f\"ğŸ’¾ ëª¨ë¸ ì €ì¥ ê²½ë¡œ: {OUTPUT_DIR}\")\n",
        "print(f\"ğŸ¤– ì‚¬ìš©í•  ë² ì´ìŠ¤ ëª¨ë¸: {BASE_MODEL}\")\n",
        "print(f\"ğŸ–¥ï¸  ë””ë°”ì´ìŠ¤: {DEVICE}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: ë°ì´í„°ì…‹ ë¡œë“œ ë° ì „ì²˜ë¦¬\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# AudioFolderë¥¼ ì‚¬ìš©í•˜ì—¬ ë°ì´í„°ì…‹ ë¡œë“œ\n",
        "print(\"ğŸ“¥ ë°ì´í„°ì…‹ì„ ë¡œë“œí•˜ëŠ” ì¤‘...\")\n",
        "\n",
        "# ë¨¼ì € í´ë”ê°€ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸\n",
        "if not os.path.exists(DATASET_PATH):\n",
        "    raise FileNotFoundError(\n",
        "        f\"ë°ì´í„°ì…‹ ê²½ë¡œê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {DATASET_PATH}\\n\"\n",
        "        \"ìœ„ì˜ 'Step 2: ì„¤ì •' ì…€ì—ì„œ DATASET_PATHë¥¼ ì˜¬ë°”ë¥´ê²Œ ì„¤ì •í•´ì£¼ì„¸ìš”!\"\n",
        "    )\n",
        "\n",
        "# í•˜ìœ„ ë””ë ‰í† ë¦¬(ë°©ì–¸ ë ˆì´ë¸”) í™•ì¸\n",
        "dialect_dirs = [d for d in os.listdir(DATASET_PATH) \n",
        "                if os.path.isdir(os.path.join(DATASET_PATH, d)) and not d.startswith('.')]\n",
        "\n",
        "if not dialect_dirs:\n",
        "    raise ValueError(\n",
        "        f\"{DATASET_PATH}ì— ë°©ì–¸ í´ë”ê°€ ì—†ìŠµë‹ˆë‹¤!\\n\"\n",
        "        \"ì˜ˆ: standard/, gyeongsang/, jeolla/, chungcheong/ ë“±\"\n",
        "    )\n",
        "\n",
        "print(f\"âœ… ê°ì§€ëœ ë°©ì–¸ ì¹´í…Œê³ ë¦¬: {dialect_dirs}\")\n",
        "\n",
        "# AudioFolderë¡œ ë°ì´í„°ì…‹ ë¡œë“œ\n",
        "dataset = load_dataset(\"audiofolder\", data_dir=DATASET_PATH)\n",
        "\n",
        "print(f\"\\nâœ… ë°ì´í„°ì…‹ ë¡œë“œ ì™„ë£Œ!\")\n",
        "print(dataset)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ë°ì´í„°ì…‹ ë¶„í•  (train/validation/test)\n",
        "print(\"\\nğŸ”€ ë°ì´í„°ì…‹ì„ train/val/testë¡œ ë¶„í• í•˜ëŠ” ì¤‘...\")\n",
        "\n",
        "# trainê³¼ ë‚˜ë¨¸ì§€ë¡œ ë¶„í•  (80% train, 20% temp)\n",
        "train_test = dataset[\"train\"].train_test_split(test_size=0.2, seed=42)\n",
        "\n",
        "# ë‚˜ë¨¸ì§€ë¥¼ validationê³¼ testë¡œ ë¶„í•  (ê° 10%ì”©)\n",
        "val_test = train_test[\"test\"].train_test_split(test_size=0.5, seed=42)\n",
        "\n",
        "# ìµœì¢… ë°ì´í„°ì…‹ êµ¬ì„±\n",
        "dataset_split = DatasetDict({\n",
        "    \"train\": train_test[\"train\"],\n",
        "    \"validation\": val_test[\"train\"],\n",
        "    \"test\": val_test[\"test\"]\n",
        "})\n",
        "\n",
        "print(\"\\nâœ… ë¶„í•  ì™„ë£Œ:\")\n",
        "print(dataset_split)\n",
        "\n",
        "# ë ˆì´ë¸” ì •ë³´ ì¶”ì¶œ\n",
        "label_names = dataset_split[\"train\"].features[\"label\"].names\n",
        "num_labels = len(label_names)\n",
        "\n",
        "print(f\"\\nğŸ“Š ë°©ì–¸ ë ˆì´ë¸” ({num_labels}ê°œ):\")\n",
        "for idx, label in enumerate(label_names):\n",
        "    train_count = sum(1 for x in dataset_split[\"train\"] if x[\"label\"] == idx)\n",
        "    val_count = sum(1 for x in dataset_split[\"validation\"] if x[\"label\"] == idx)\n",
        "    test_count = sum(1 for x in dataset_split[\"test\"] if x[\"label\"] == idx)\n",
        "    print(f\"  {idx}. {label:15s} - Train: {train_count:3d}, Val: {val_count:3d}, Test: {test_count:3d}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4: ëª¨ë¸ ë° í”„ë¡œì„¸ì„œ ë¡œë“œ\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(f\"\\nğŸ¤– ëª¨ë¸ê³¼ í”„ë¡œì„¸ì„œë¥¼ ë¡œë“œí•˜ëŠ” ì¤‘: {BASE_MODEL}\")\n",
        "\n",
        "# Processor ë¡œë“œ (ì˜¤ë””ì˜¤ ì „ì²˜ë¦¬ìš©)\n",
        "processor = Wav2Vec2Processor.from_pretrained(BASE_MODEL)\n",
        "\n",
        "# ëª¨ë¸ ë¡œë“œ (ë¶„ë¥˜ í—¤ë“œ ì¶”ê°€)\n",
        "model = Wav2Vec2ForSequenceClassification.from_pretrained(\n",
        "    BASE_MODEL,\n",
        "    num_labels=num_labels,\n",
        "    label2id={label: i for i, label in enumerate(label_names)},\n",
        "    id2label={i: label for i, label in enumerate(label_names)},\n",
        ")\n",
        "\n",
        "# ëª¨ë¸ì„ GPUë¡œ ì´ë™ (ê°€ëŠ¥í•œ ê²½ìš°)\n",
        "model = model.to(DEVICE)\n",
        "\n",
        "print(f\"âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ! (íŒŒë¼ë¯¸í„° ìˆ˜: {model.num_parameters():,})\")\n",
        "print(f\"âœ… í”„ë¡œì„¸ì„œ ë¡œë“œ ì™„ë£Œ!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 5: ë°ì´í„° ì „ì²˜ë¦¬ í•¨ìˆ˜\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def preprocess_function(examples):\n",
        "    \"\"\"\n",
        "    ì˜¤ë””ì˜¤ ë°ì´í„°ë¥¼ ëª¨ë¸ ì…ë ¥ í˜•íƒœë¡œ ë³€í™˜\n",
        "    \"\"\"\n",
        "    # ì˜¤ë””ì˜¤ë¥¼ 16kHzë¡œ ë¦¬ìƒ˜í”Œë§\n",
        "    audio_arrays = [x[\"array\"] for x in examples[\"audio\"]]\n",
        "    \n",
        "    # í”„ë¡œì„¸ì„œë¡œ ë³€í™˜ (ìë™ìœ¼ë¡œ íŒ¨ë”© ë° ì •ê·œí™”)\n",
        "    inputs = processor(\n",
        "        audio_arrays,\n",
        "        sampling_rate=processor.feature_extractor.sampling_rate,\n",
        "        return_tensors=\"pt\",\n",
        "        padding=True,\n",
        "        max_length=int(processor.feature_extractor.sampling_rate * MAX_DURATION),\n",
        "        truncation=True,\n",
        "    )\n",
        "    \n",
        "    return {\n",
        "        \"input_values\": inputs.input_values,\n",
        "        \"labels\": examples[\"label\"]\n",
        "    }\n",
        "\n",
        "print(\"âš™ï¸ ë°ì´í„° ì „ì²˜ë¦¬ ì¤‘...\")\n",
        "print(\"ì´ ì‘ì—…ì€ ì‹œê°„ì´ ê±¸ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤ (ìˆ˜ ë¶„ ~ ìˆ˜ì‹­ ë¶„)...\")\n",
        "\n",
        "# ì˜¤ë””ì˜¤ë¥¼ 16kHzë¡œ ë¦¬ìƒ˜í”Œë§\n",
        "dataset_split = dataset_split.cast_column(\"audio\", Audio(sampling_rate=SAMPLE_RATE))\n",
        "\n",
        "# ì „ì²˜ë¦¬ ì ìš© (ë°°ì¹˜ ì²˜ë¦¬)\n",
        "encoded_dataset = dataset_split.map(\n",
        "    preprocess_function,\n",
        "    batched=True,\n",
        "    batch_size=8,\n",
        "    remove_columns=dataset_split[\"train\"].column_names,\n",
        ")\n",
        "\n",
        "print(\"\\nâœ… ë°ì´í„° ì „ì²˜ë¦¬ ì™„ë£Œ!\")\n",
        "print(encoded_dataset)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 6: í‰ê°€ ë©”íŠ¸ë¦­ ì •ì˜\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Accuracy ë©”íŠ¸ë¦­ ë¡œë“œ\n",
        "metric = evaluate.load(\"accuracy\")\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    \"\"\"\n",
        "    í•™ìŠµ ì¤‘ í‰ê°€ ë©”íŠ¸ë¦­ ê³„ì‚°\n",
        "    \"\"\"\n",
        "    predictions = np.argmax(eval_pred.predictions, axis=1)\n",
        "    return metric.compute(predictions=predictions, references=eval_pred.label_ids)\n",
        "\n",
        "print(\"âœ… í‰ê°€ ë©”íŠ¸ë¦­ ì¤€ë¹„ ì™„ë£Œ!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 7: í•™ìŠµ ì„¤ì •\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "training_args = TrainingArguments(\n",
        "    output_dir=OUTPUT_DIR,\n",
        "    per_device_train_batch_size=BATCH_SIZE,\n",
        "    per_device_eval_batch_size=BATCH_SIZE,\n",
        "    gradient_accumulation_steps=2,  # ë©”ëª¨ë¦¬ê°€ ë¶€ì¡±í•˜ë©´ ì´ ê°’ì„ ë†’ì´ì„¸ìš”\n",
        "    eval_strategy=\"steps\",\n",
        "    eval_steps=EVAL_STEPS,\n",
        "    save_steps=SAVE_STEPS,\n",
        "    save_total_limit=3,  # ìµœëŒ€ 3ê°œì˜ ì²´í¬í¬ì¸íŠ¸ë§Œ ìœ ì§€\n",
        "    num_train_epochs=NUM_EPOCHS,\n",
        "    learning_rate=LEARNING_RATE,\n",
        "    warmup_ratio=0.1,\n",
        "    logging_dir=f\"{OUTPUT_DIR}/logs\",\n",
        "    logging_steps=50,\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"accuracy\",\n",
        "    greater_is_better=True,\n",
        "    push_to_hub=False,\n",
        "    fp16=USE_GPU,  # GPU ì‚¬ìš© ì‹œ mixed precision í™œì„±í™”\n",
        "    dataloader_num_workers=4,\n",
        "    remove_unused_columns=False,\n",
        ")\n",
        "\n",
        "print(\"\\nğŸ“‹ í•™ìŠµ ì„¤ì •:\")\n",
        "print(f\"  â€¢ Batch size: {BATCH_SIZE}\")\n",
        "print(f\"  â€¢ Epochs: {NUM_EPOCHS}\")\n",
        "print(f\"  â€¢ Learning rate: {LEARNING_RATE}\")\n",
        "print(f\"  â€¢ Mixed precision (FP16): {USE_GPU}\")\n",
        "print(f\"  â€¢ Output directory: {OUTPUT_DIR}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 8: Trainer ì´ˆê¸°í™” ë° í•™ìŠµ ì‹œì‘\n",
        "\n",
        "**ì£¼ì˜**: ë‹¤ìŒ ì…€ì€ ì‹¤í–‰ ì‹œê°„ì´ ì˜¤ë˜ ê±¸ë¦½ë‹ˆë‹¤ (GPU ì‚¬ìš© ì‹œ 1-3ì‹œê°„, CPU ì‚¬ìš© ì‹œ ìˆ˜ ì‹œê°„~ìˆ˜ì¼)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Early Stopping ì„¤ì • (validation ì„±ëŠ¥ì´ ê°œì„ ë˜ì§€ ì•Šìœ¼ë©´ ì¡°ê¸° ì¢…ë£Œ)\n",
        "early_stopping = EarlyStoppingCallback(\n",
        "    early_stopping_patience=3,  # 3ë²ˆ ì—°ì† ê°œì„  ì—†ìœ¼ë©´ ì¢…ë£Œ\n",
        "    early_stopping_threshold=0.01\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=encoded_dataset[\"train\"],\n",
        "    eval_dataset=encoded_dataset[\"validation\"],\n",
        "    compute_metrics=compute_metrics,\n",
        "    callbacks=[early_stopping],\n",
        ")\n",
        "\n",
        "print(\"\\nâœ… Trainer ì´ˆê¸°í™” ì™„ë£Œ!\")\n",
        "print(\"\\nğŸš€ í•™ìŠµì„ ì‹œì‘í•©ë‹ˆë‹¤...\\n\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# í•™ìŠµ ì‹œì‘\n",
        "train_result = trainer.train()\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"\\nâœ… í•™ìŠµ ì™„ë£Œ!\\n\")\n",
        "print(\"í•™ìŠµ ê²°ê³¼:\")\n",
        "print(train_result)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 9: ëª¨ë¸ ì €ì¥\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\nğŸ’¾ ìµœì¢… ëª¨ë¸ ì €ì¥ ì¤‘...\")\n",
        "\n",
        "# ìµœì¢… ëª¨ë¸ ì €ì¥\n",
        "final_model_path = f\"{OUTPUT_DIR}/final_model\"\n",
        "trainer.save_model(final_model_path)\n",
        "processor.save_pretrained(final_model_path)\n",
        "\n",
        "print(f\"\\nâœ… ëª¨ë¸ì´ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤: {final_model_path}\")\n",
        "print(\"\\nì €ì¥ëœ íŒŒì¼:\")\n",
        "for file in os.listdir(final_model_path):\n",
        "    print(f\"  â€¢ {file}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 10: í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ í‰ê°€ ë° ê²°ê³¼ ë¶„ì„\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\nğŸ“Š í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ í‰ê°€ ì¤‘...\\n\")\n",
        "\n",
        "# í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ í‰ê°€\n",
        "test_results = trainer.evaluate(encoded_dataset[\"test\"])\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"í…ŒìŠ¤íŠ¸ ê²°ê³¼:\")\n",
        "print(\"=\" * 80)\n",
        "for key, value in test_results.items():\n",
        "    print(f\"  {key}: {value:.4f}\")\n",
        "\n",
        "# í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ ì˜ˆì¸¡\n",
        "predictions = trainer.predict(encoded_dataset[\"test\"])\n",
        "predicted_labels = np.argmax(predictions.predictions, axis=1)\n",
        "true_labels = predictions.label_ids\n",
        "\n",
        "# ìƒì„¸ ë¶„ë¥˜ ë¦¬í¬íŠ¸\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"ìƒì„¸ ë¶„ë¥˜ ë¦¬í¬íŠ¸:\")\n",
        "print(\"=\" * 80)\n",
        "print(classification_report(\n",
        "    true_labels, \n",
        "    predicted_labels, \n",
        "    target_names=label_names,\n",
        "    digits=4\n",
        "))\n",
        "\n",
        "# Confusion Matrix\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"Confusion Matrix:\")\n",
        "print(\"=\" * 80)\n",
        "cm = confusion_matrix(true_labels, predicted_labels)\n",
        "cm_df = pd.DataFrame(cm, index=label_names, columns=label_names)\n",
        "print(cm_df)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 11: ëª¨ë¸ í…ŒìŠ¤íŠ¸ (ì‹¤ì œ ì˜¤ë””ì˜¤ íŒŒì¼ë¡œ ì¶”ë¡ )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import pipeline\n",
        "import tempfile\n",
        "\n",
        "print(\"\\nğŸ§ª ì¶”ë¡  íŒŒì´í”„ë¼ì¸ ìƒì„± ì¤‘...\")\n",
        "\n",
        "# ì¶”ë¡  íŒŒì´í”„ë¼ì¸ ìƒì„±\n",
        "classifier = pipeline(\n",
        "    \"audio-classification\",\n",
        "    model=final_model_path,\n",
        "    device=0 if USE_GPU else -1\n",
        ")\n",
        "\n",
        "print(\"âœ… íŒŒì´í”„ë¼ì¸ ì¤€ë¹„ ì™„ë£Œ!\\n\")\n",
        "\n",
        "# í…ŒìŠ¤íŠ¸ ìƒ˜í”Œë¡œ ì¶”ë¡  (í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ì˜ ì²« 5ê°œ)\n",
        "print(\"=\" * 80)\n",
        "print(\"ìƒ˜í”Œ ì¶”ë¡  í…ŒìŠ¤íŠ¸:\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "test_samples = dataset_split[\"test\"].select(range(min(5, len(dataset_split[\"test\"]))))\n",
        "\n",
        "for idx, sample in enumerate(test_samples):\n",
        "    audio_array = sample[\"audio\"][\"array\"]\n",
        "    true_label = label_names[sample[\"label\"]]\n",
        "    \n",
        "    # ì„ì‹œ íŒŒì¼ë¡œ ì €ì¥ (pipelineì€ íŒŒì¼ ê²½ë¡œ ë˜ëŠ” arrayë¥¼ ë°›ìŒ)\n",
        "    with tempfile.NamedTemporaryFile(suffix=\".wav\", delete=False) as temp_file:\n",
        "        sf.write(temp_file.name, audio_array, SAMPLE_RATE)\n",
        "        temp_path = temp_file.name\n",
        "    \n",
        "    # ì¶”ë¡ \n",
        "    result = classifier(temp_path, top_k=len(label_names))\n",
        "    \n",
        "    # ê²°ê³¼ ì¶œë ¥\n",
        "    print(f\"\\nìƒ˜í”Œ {idx + 1}:\")\n",
        "    print(f\"  ì‹¤ì œ ë ˆì´ë¸”: {true_label}\")\n",
        "    print(f\"  ì˜ˆì¸¡ ê²°ê³¼:\")\n",
        "    for pred in result:\n",
        "        print(f\"    - {pred['label']:15s}: {pred['score']:.4f}\")\n",
        "    \n",
        "    # ì„ì‹œ íŒŒì¼ ì‚­ì œ\n",
        "    os.remove(temp_path)\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ğŸ‰ ì™„ë£Œ! ë‹¤ìŒ ë‹¨ê³„\n",
        "\n",
        "### âœ… ì™„ë£Œëœ ì‘ì—…\n",
        "1. ë°ì´í„°ì…‹ ë¡œë“œ ë° ì „ì²˜ë¦¬\n",
        "2. Wav2Vec2 ëª¨ë¸ íŒŒì¸íŠœë‹\n",
        "3. ëª¨ë¸ í‰ê°€ ë° ì €ì¥\n",
        "4. ì¶”ë¡  í…ŒìŠ¤íŠ¸\n",
        "\n",
        "### ğŸ“¦ ìƒì„±ëœ íŒŒì¼\n",
        "- ëª¨ë¸ íŒŒì¼: `../models/dialect_classifier/final_model/`\n",
        "- ì²´í¬í¬ì¸íŠ¸: `../models/dialect_classifier/checkpoint-*/`\n",
        "- ë¡œê·¸: `../models/dialect_classifier/logs/`\n",
        "\n",
        "### ğŸš€ ë‹¤ìŒ ë‹¨ê³„\n",
        "\n",
        "**HabitLink ì• í”Œë¦¬ì¼€ì´ì…˜ì—ì„œ ë°©ì–¸ ë¶„ì„ ì‚¬ìš©í•˜ê¸°:**\n",
        "\n",
        "1. ëª¨ë¸ì´ ìë™ìœ¼ë¡œ `src/dialect_analyzer.py`ì— í†µí•©ë˜ì—ˆìŠµë‹ˆë‹¤\n",
        "2. ë©”ì¸ ì• í”Œë¦¬ì¼€ì´ì…˜ ì‹¤í–‰: `python main.py`\n",
        "3. ë¶„ì„ ëª¨ë“ˆ ì„ íƒ ì‹œ \"ë°©ì–¸ ë¶„ì„\"ì„ ì„ íƒí•˜ë©´ ë©ë‹ˆë‹¤!\n",
        "\n",
        "**ëª¨ë¸ ê°œì„  (ì„ íƒì‚¬í•­):**\n",
        "- ë” ë§ì€ ë°ì´í„° ìˆ˜ì§‘\n",
        "- Hyperparameter íŠœë‹ (learning rate, batch size ë“±)\n",
        "- Data augmentation ì ìš© (ë…¸ì´ì¦ˆ ì¶”ê°€, ì†ë„ ë³€ê²½ ë“±)\n",
        "\n",
        "**í”„ë¡œê·¸ë˜ë° ë°©ì‹ìœ¼ë¡œ ì‚¬ìš©:**\n",
        "```python\n",
        "from src.dialect_analyzer import DialectAnalyzer\n",
        "\n",
        "analyzer = DialectAnalyzer(\"../models/dialect_classifier/final_model\")\n",
        "result = analyzer.analyze(\"path/to/audio.wav\")\n",
        "print(result)  # {'Gyeongsang': 0.85, 'Seoul': 0.10, ...}\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"ğŸ‰ ëª¨ë“  ì‘ì—…ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!\")\n",
        "print(\"=\"*80)\n",
        "print(f\"\\nëª¨ë¸ ê²½ë¡œ: {final_model_path}\")\n",
        "print(\"\\nì´ì œ HabitLink ë©”ì¸ ì• í”Œë¦¬ì¼€ì´ì…˜ì—ì„œ ë°©ì–¸ ë¶„ì„ì„ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤!\")\n",
        "print(\"\\në‹¤ìŒ ëª…ë ¹ì–´ë¡œ í…ŒìŠ¤íŠ¸í•˜ì„¸ìš”:\")\n",
        "print(\"  cd .. && python main.py\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
