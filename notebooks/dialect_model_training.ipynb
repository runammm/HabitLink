{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# í•œêµ­ì–´ í‘œì¤€ì–´ vs ë¹„í‘œì¤€ì–´ ì´ì§„ ë¶„ë¥˜ ëª¨ë¸ í•™ìŠµ\n",
        "\n",
        "ì´ ë…¸íŠ¸ë¶ì€ Wav2Vec2 ëª¨ë¸ì„ **ì´ì§„ ë¶„ë¥˜(í‘œì¤€ì–´ vs ë¹„í‘œì¤€ì–´)**ì— ë§ê²Œ íŒŒì¸íŠœë‹í•©ë‹ˆë‹¤.\n",
        "\n",
        "**ğŸ¯ ëª©ì **: í‘œì¤€ì–´ì¸ì§€ ì•„ë‹Œì§€ë§Œ íŒë³„ (ê°„ë‹¨í•˜ê³  ì‹¤ìš©ì !)\n",
        "\n",
        "## Google Colabì—ì„œ ì‹¤í–‰í•˜ê¸°\n",
        "\n",
        "### âœ… ì´ ë…¸íŠ¸ë¶ì€ Google Colabì—ì„œ ë°”ë¡œ ì‹¤í–‰ ê°€ëŠ¥í•©ë‹ˆë‹¤!\n",
        "\n",
        "1. ì´ ë…¸íŠ¸ë¶ì„ Colabì— ì—…ë¡œë“œ\n",
        "2. GPU ëŸ°íƒ€ì„ ì„¤ì •: `ëŸ°íƒ€ì„ > ëŸ°íƒ€ì„ ìœ í˜• ë³€ê²½ > T4 GPU`\n",
        "3. ë°ì´í„°ë¥¼ Google Driveì— ì—…ë¡œë“œ\n",
        "4. ì•„ë˜ ì…€ë“¤ì„ ìˆœì„œëŒ€ë¡œ ì‹¤í–‰\n",
        "\n",
        "---\n",
        "\n",
        "## í•„ìš”í•œ ì‚¬ì „ ì¤€ë¹„\n",
        "\n",
        "### 1. ë°ì´í„°ì…‹ êµ¬ì¡°\n",
        "**ì´ì§„ ë¶„ë¥˜ìš©**: í‘œì¤€ì–´ í´ë” + ë¹„í‘œì¤€ì–´ í´ë”ë“¤\n",
        "\n",
        "```\n",
        "/path/to/your/dialect_dataset/\n",
        "â”œâ”€â”€ standard/          # í‘œì¤€ì–´ â†’ Label: 0 (í‘œì¤€ì–´)\n",
        "â”‚   â”œâ”€â”€ audio_001.wav\n",
        "â”‚   â”œâ”€â”€ audio_002.wav\n",
        "â”‚   â””â”€â”€ ...\n",
        "â”œâ”€â”€ gyeongsang/        # ê²½ìƒë„ ë°©ì–¸ â†’ Label: 1 (ë¹„í‘œì¤€ì–´)\n",
        "â”‚   â”œâ”€â”€ audio_001.wav\n",
        "â”‚   â””â”€â”€ ...\n",
        "â””â”€â”€ gangwon/           # ê°•ì›ë„ ë°©ì–¸ â†’ Label: 1 (ë¹„í‘œì¤€ì–´)\n",
        "    â”œâ”€â”€ audio_001.wav\n",
        "    â””â”€â”€ ...\n",
        "```\n",
        "\n",
        "**ğŸ’¡ ì¤‘ìš”**: \n",
        "- `standard` í´ë”: í‘œì¤€ì–´ (ì„œìš¸/ìˆ˜ë„ê¶Œ)\n",
        "- ë‚˜ë¨¸ì§€ ëª¨ë“  í´ë”: ë¹„í‘œì¤€ì–´ë¡œ í†µí•© ì²˜ë¦¬ë¨\n",
        "- í‘œì¤€ì–´ì™€ ë¹„í‘œì¤€ì–´ ë°ì´í„° ë¹„ìœ¨ì„ ë¹„ìŠ·í•˜ê²Œ ìœ ì§€í•˜ì„¸ìš”\n",
        "\n",
        "### 2. ì˜¤ë””ì˜¤ íŒŒì¼ ìš”êµ¬ì‚¬í•­\n",
        "- í˜•ì‹: WAV ë˜ëŠ” MP3\n",
        "- ìƒ˜í”Œë§ ë ˆì´íŠ¸: 16kHz ê¶Œì¥ (ìë™ ë¦¬ìƒ˜í”Œë§ë¨)\n",
        "- ê¸¸ì´: 2ì´ˆ ~ 30ì´ˆ ê¶Œì¥\n",
        "- í‘œì¤€ì–´/ë¹„í‘œì¤€ì–´ ê°ê° ìµœì†Œ 100ê°œ ì´ìƒ ìƒ˜í”Œ ê¶Œì¥\n",
        "\n",
        "### 3. í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬\n",
        "Colabì—ì„œ ì²« ë²ˆì§¸ ì…€ë¡œ ì„¤ì¹˜ë©ë‹ˆë‹¤ (ìë™)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: í™˜ê²½ ì„¤ì • ë° ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Google Colabìš© ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜\n",
        "print(\"ğŸ“¦ í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜ ì¤‘...\")\n",
        "%pip install -q transformers datasets torch torchaudio accelerate evaluate scikit-learn soundfile librosa\n",
        "\n",
        "print(\"\\nâœ… ì„¤ì¹˜ ì™„ë£Œ!\\n\")\n",
        "\n",
        "import os\n",
        "import json\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "from typing import Dict, List, Tuple\n",
        "\n",
        "# Hugging Face libraries\n",
        "from datasets import load_dataset, Audio, DatasetDict\n",
        "from transformers import (\n",
        "    Wav2Vec2Processor,\n",
        "    Wav2Vec2ForSequenceClassification,\n",
        "    TrainingArguments,\n",
        "    Trainer,\n",
        ")\n",
        "from transformers import EarlyStoppingCallback\n",
        "\n",
        "# Evaluation\n",
        "import evaluate\n",
        "\n",
        "# Audio processing\n",
        "import librosa\n",
        "import soundfile as sf\n",
        "\n",
        "# ML utilities\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import torch\n",
        "\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")\n",
        "    \n",
        "# Google Colab í™˜ê²½ í™•ì¸\n",
        "try:\n",
        "    import google.colab\n",
        "    IN_COLAB = True\n",
        "    print(f\"âœ… Google Colab í™˜ê²½ì—ì„œ ì‹¤í–‰ ì¤‘\")\n",
        "except:\n",
        "    IN_COLAB = False\n",
        "    print(f\"ğŸ’» ë¡œì»¬ í™˜ê²½ì—ì„œ ì‹¤í–‰ ì¤‘\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: ì„¤ì • (ì—¬ê¸°ë¥¼ ìˆ˜ì •í•˜ì„¸ìš”!)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================\n",
        "# ğŸ”§ ì—¬ê¸°ë¥¼ ë³¸ì¸ì˜ í™˜ê²½ì— ë§ê²Œ ìˆ˜ì •í•˜ì„¸ìš”!\n",
        "# ============================================\n",
        "\n",
        "# Google Drive ë§ˆìš´íŠ¸ (Colab ì‚¬ìš© ì‹œ)\n",
        "if IN_COLAB:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "    print(\"âœ… Google Drive ë§ˆìš´íŠ¸ ì™„ë£Œ!\")\n",
        "    \n",
        "    # Colabì—ì„œì˜ ê¸°ë³¸ ê²½ë¡œ ì„¤ì •\n",
        "    DATASET_PATH = \"/content/drive/MyDrive/dialect_dataset\"  # ì—¬ê¸°ë¥¼ ìˆ˜ì •!\n",
        "    OUTPUT_DIR = \"/content/drive/MyDrive/models/dialect_binary_classifier\"\n",
        "else:\n",
        "    # ë¡œì»¬ í™˜ê²½ ê²½ë¡œ\n",
        "    DATASET_PATH = \"/path/to/your/ssd/dialect_dataset\"  # ì—¬ê¸°ë¥¼ ìˆ˜ì •!\n",
        "    OUTPUT_DIR = \"../models/dialect_binary_classifier\"\n",
        "\n",
        "print(f\"\\nğŸ“‚ ë°ì´í„°ì…‹ ê²½ë¡œ: {DATASET_PATH}\")\n",
        "print(f\"ğŸ’¾ ëª¨ë¸ ì €ì¥ ê²½ë¡œ: {OUTPUT_DIR}\")\n",
        "\n",
        "# ì‚¬ì „ í•™ìŠµëœ í•œêµ­ì–´ Wav2Vec2 ëª¨ë¸\n",
        "# ì˜µì…˜ 1: kresnik/wav2vec2-large-xlsr-korean (ì¶”ì²œ)\n",
        "# ì˜µì…˜ 2: facebook/wav2vec2-large-xlsr-53 (ë‹¤êµ­ì–´)\n",
        "BASE_MODEL = \"kresnik/wav2vec2-large-xlsr-korean\"\n",
        "\n",
        "# í•™ìŠµ ì„¤ì • (Colab T4 GPU ê¸°ì¤€ ìµœì í™”)\n",
        "BATCH_SIZE = 16 if IN_COLAB else 8  # Colab GPUëŠ” ë” í° ë°°ì¹˜ ê°€ëŠ¥\n",
        "NUM_EPOCHS = 10\n",
        "LEARNING_RATE = 3e-5\n",
        "EVAL_STEPS = 50  # ë” ìì£¼ í‰ê°€\n",
        "SAVE_STEPS = 50  # ë” ìì£¼ ì €ì¥\n",
        "\n",
        "# ì˜¤ë””ì˜¤ ì„¤ì •\n",
        "SAMPLE_RATE = 16000\n",
        "MAX_DURATION = 30  # ì´ˆ ë‹¨ìœ„ (30ì´ˆ ì´ìƒì€ ì˜ë¦¼)\n",
        "\n",
        "# GPU ì‚¬ìš© ì„¤ì •\n",
        "USE_GPU = torch.cuda.is_available()\n",
        "DEVICE = \"cuda\" if USE_GPU else \"cpu\"\n",
        "\n",
        "print(f\"ğŸ¤– ë² ì´ìŠ¤ ëª¨ë¸: {BASE_MODEL}\")\n",
        "print(f\"ğŸ–¥ï¸  ë””ë°”ì´ìŠ¤: {DEVICE}\")\n",
        "print(f\"ğŸ“Š ë°°ì¹˜ í¬ê¸°: {BATCH_SIZE}\")\n",
        "print(f\"âš¡ Colab ìµœì í™”: {'ON' if IN_COLAB else 'OFF'}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: ë°ì´í„°ì…‹ ë¡œë“œ ë° ì „ì²˜ë¦¬\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# AudioFolderë¥¼ ì‚¬ìš©í•˜ì—¬ ë°ì´í„°ì…‹ ë¡œë“œ\n",
        "print(\"ğŸ“¥ ë°ì´í„°ì…‹ì„ ë¡œë“œí•˜ëŠ” ì¤‘...\")\n",
        "\n",
        "# ë¨¼ì € í´ë”ê°€ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸\n",
        "if not os.path.exists(DATASET_PATH):\n",
        "    raise FileNotFoundError(\n",
        "        f\"ë°ì´í„°ì…‹ ê²½ë¡œê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {DATASET_PATH}\\n\"\n",
        "        \"ìœ„ì˜ 'Step 2: ì„¤ì •' ì…€ì—ì„œ DATASET_PATHë¥¼ ì˜¬ë°”ë¥´ê²Œ ì„¤ì •í•´ì£¼ì„¸ìš”!\"\n",
        "    )\n",
        "\n",
        "# í•˜ìœ„ ë””ë ‰í† ë¦¬(ë°©ì–¸ ë ˆì´ë¸”) í™•ì¸\n",
        "dialect_dirs = [d for d in os.listdir(DATASET_PATH) \n",
        "                if os.path.isdir(os.path.join(DATASET_PATH, d)) and not d.startswith('.')]\n",
        "\n",
        "if not dialect_dirs:\n",
        "    raise ValueError(\n",
        "        f\"{DATASET_PATH}ì— ë°©ì–¸ í´ë”ê°€ ì—†ìŠµë‹ˆë‹¤!\\n\"\n",
        "        \"ì˜ˆ: standard/, gyeongsang/, jeolla/, chungcheong/ ë“±\"\n",
        "    )\n",
        "\n",
        "print(f\"âœ… ê°ì§€ëœ ë°©ì–¸ ì¹´í…Œê³ ë¦¬: {dialect_dirs}\")\n",
        "\n",
        "# AudioFolderë¡œ ë°ì´í„°ì…‹ ë¡œë“œ\n",
        "dataset = load_dataset(\"audiofolder\", data_dir=DATASET_PATH)\n",
        "\n",
        "print(f\"\\nâœ… ë°ì´í„°ì…‹ ë¡œë“œ ì™„ë£Œ!\")\n",
        "print(dataset)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ğŸ¯ ì´ì§„ ë¶„ë¥˜ë¥¼ ìœ„í•œ ë ˆì´ë¸” ë³€í™˜\n",
        "print(\"\\nğŸ”„ ì´ì§„ ë¶„ë¥˜ë¥¼ ìœ„í•œ ë ˆì´ë¸” ë³€í™˜ ì¤‘...\")\n",
        "print(\"  â€¢ standard â†’ 0 (í‘œì¤€ì–´)\")\n",
        "print(\"  â€¢ ë‚˜ë¨¸ì§€ â†’ 1 (ë¹„í‘œì¤€ì–´)\")\n",
        "\n",
        "def convert_to_binary_label(example):\n",
        "    \"\"\"\n",
        "    ì´ì§„ ë¶„ë¥˜ë¡œ ë³€í™˜: standard=0, ë‚˜ë¨¸ì§€=1\n",
        "    \"\"\"\n",
        "    original_label_name = dataset[\"train\"].features[\"label\"].names[example[\"label\"]]\n",
        "    # standardëŠ” 0, ë‚˜ë¨¸ì§€ëŠ” ëª¨ë‘ 1\n",
        "    example[\"label\"] = 0 if original_label_name == \"standard\" else 1\n",
        "    return example\n",
        "\n",
        "# ë ˆì´ë¸” ë³€í™˜ ì ìš©\n",
        "dataset = dataset.map(convert_to_binary_label)\n",
        "\n",
        "print(\"âœ… ë ˆì´ë¸” ë³€í™˜ ì™„ë£Œ!\")\n",
        "\n",
        "# ë°ì´í„°ì…‹ ë¶„í•  (train/validation/test)\n",
        "print(\"\\nğŸ”€ ë°ì´í„°ì…‹ì„ train/val/testë¡œ ë¶„í• í•˜ëŠ” ì¤‘...\")\n",
        "\n",
        "# trainê³¼ ë‚˜ë¨¸ì§€ë¡œ ë¶„í•  (80% train, 20% temp)\n",
        "train_test = dataset[\"train\"].train_test_split(test_size=0.2, seed=42, stratify_by_column=\"label\")\n",
        "\n",
        "# ë‚˜ë¨¸ì§€ë¥¼ validationê³¼ testë¡œ ë¶„í•  (ê° 10%ì”©)\n",
        "val_test = train_test[\"test\"].train_test_split(test_size=0.5, seed=42, stratify_by_column=\"label\")\n",
        "\n",
        "# ìµœì¢… ë°ì´í„°ì…‹ êµ¬ì„±\n",
        "dataset_split = DatasetDict({\n",
        "    \"train\": train_test[\"train\"],\n",
        "    \"validation\": val_test[\"train\"],\n",
        "    \"test\": val_test[\"test\"]\n",
        "})\n",
        "\n",
        "print(\"\\nâœ… ë¶„í•  ì™„ë£Œ:\")\n",
        "print(dataset_split)\n",
        "\n",
        "# ì´ì§„ ë¶„ë¥˜ ë ˆì´ë¸” ì •ì˜\n",
        "label_names = [\"standard\", \"non_standard\"]\n",
        "num_labels = 2\n",
        "\n",
        "print(f\"\\nğŸ“Š ì´ì§„ ë¶„ë¥˜ ë ˆì´ë¸” ë¶„í¬:\")\n",
        "for idx, label in enumerate(label_names):\n",
        "    train_count = sum(1 for x in dataset_split[\"train\"] if x[\"label\"] == idx)\n",
        "    val_count = sum(1 for x in dataset_split[\"validation\"] if x[\"label\"] == idx)\n",
        "    test_count = sum(1 for x in dataset_split[\"test\"] if x[\"label\"] == idx)\n",
        "    label_kr = \"í‘œì¤€ì–´\" if idx == 0 else \"ë¹„í‘œì¤€ì–´\"\n",
        "    print(f\"  {idx}. {label_kr:10s} ({label:15s}) - Train: {train_count:3d}, Val: {val_count:3d}, Test: {test_count:3d}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4: ëª¨ë¸ ë° í”„ë¡œì„¸ì„œ ë¡œë“œ\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(f\"\\nğŸ¤– ëª¨ë¸ê³¼ í”„ë¡œì„¸ì„œë¥¼ ë¡œë“œí•˜ëŠ” ì¤‘: {BASE_MODEL}\")\n",
        "\n",
        "# Processor ë¡œë“œ (ì˜¤ë””ì˜¤ ì „ì²˜ë¦¬ìš©)\n",
        "processor = Wav2Vec2Processor.from_pretrained(BASE_MODEL)\n",
        "\n",
        "# ëª¨ë¸ ë¡œë“œ (ë¶„ë¥˜ í—¤ë“œ ì¶”ê°€)\n",
        "model = Wav2Vec2ForSequenceClassification.from_pretrained(\n",
        "    BASE_MODEL,\n",
        "    num_labels=num_labels,\n",
        "    label2id={label: i for i, label in enumerate(label_names)},\n",
        "    id2label={i: label for i, label in enumerate(label_names)},\n",
        ")\n",
        "\n",
        "# ëª¨ë¸ì„ GPUë¡œ ì´ë™ (ê°€ëŠ¥í•œ ê²½ìš°)\n",
        "model = model.to(DEVICE)\n",
        "\n",
        "print(f\"âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ! (íŒŒë¼ë¯¸í„° ìˆ˜: {model.num_parameters():,})\")\n",
        "print(f\"âœ… í”„ë¡œì„¸ì„œ ë¡œë“œ ì™„ë£Œ!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 5: ë°ì´í„° ì „ì²˜ë¦¬ í•¨ìˆ˜\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def preprocess_function(examples):\n",
        "    \"\"\"\n",
        "    ì˜¤ë””ì˜¤ ë°ì´í„°ë¥¼ ëª¨ë¸ ì…ë ¥ í˜•íƒœë¡œ ë³€í™˜\n",
        "    \"\"\"\n",
        "    # ì˜¤ë””ì˜¤ë¥¼ 16kHzë¡œ ë¦¬ìƒ˜í”Œë§\n",
        "    audio_arrays = [x[\"array\"] for x in examples[\"audio\"]]\n",
        "    \n",
        "    # í”„ë¡œì„¸ì„œë¡œ ë³€í™˜ (ìë™ìœ¼ë¡œ íŒ¨ë”© ë° ì •ê·œí™”)\n",
        "    inputs = processor(\n",
        "        audio_arrays,\n",
        "        sampling_rate=processor.feature_extractor.sampling_rate,\n",
        "        return_tensors=\"pt\",\n",
        "        padding=True,\n",
        "        max_length=int(processor.feature_extractor.sampling_rate * MAX_DURATION),\n",
        "        truncation=True,\n",
        "    )\n",
        "    \n",
        "    return {\n",
        "        \"input_values\": inputs.input_values,\n",
        "        \"labels\": examples[\"label\"]\n",
        "    }\n",
        "\n",
        "print(\"âš™ï¸ ë°ì´í„° ì „ì²˜ë¦¬ ì¤‘...\")\n",
        "print(\"ì´ ì‘ì—…ì€ ì‹œê°„ì´ ê±¸ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤ (ìˆ˜ ë¶„ ~ ìˆ˜ì‹­ ë¶„)...\")\n",
        "\n",
        "# ì˜¤ë””ì˜¤ë¥¼ 16kHzë¡œ ë¦¬ìƒ˜í”Œë§\n",
        "dataset_split = dataset_split.cast_column(\"audio\", Audio(sampling_rate=SAMPLE_RATE))\n",
        "\n",
        "# ì „ì²˜ë¦¬ ì ìš© (ë°°ì¹˜ ì²˜ë¦¬)\n",
        "encoded_dataset = dataset_split.map(\n",
        "    preprocess_function,\n",
        "    batched=True,\n",
        "    batch_size=8,\n",
        "    remove_columns=dataset_split[\"train\"].column_names,\n",
        ")\n",
        "\n",
        "print(\"\\nâœ… ë°ì´í„° ì „ì²˜ë¦¬ ì™„ë£Œ!\")\n",
        "print(encoded_dataset)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 6: í‰ê°€ ë©”íŠ¸ë¦­ ì •ì˜\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Accuracy ë©”íŠ¸ë¦­ ë¡œë“œ\n",
        "metric = evaluate.load(\"accuracy\")\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    \"\"\"\n",
        "    í•™ìŠµ ì¤‘ í‰ê°€ ë©”íŠ¸ë¦­ ê³„ì‚°\n",
        "    \"\"\"\n",
        "    predictions = np.argmax(eval_pred.predictions, axis=1)\n",
        "    return metric.compute(predictions=predictions, references=eval_pred.label_ids)\n",
        "\n",
        "print(\"âœ… í‰ê°€ ë©”íŠ¸ë¦­ ì¤€ë¹„ ì™„ë£Œ!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 7: í•™ìŠµ ì„¤ì •\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "training_args = TrainingArguments(\n",
        "    output_dir=OUTPUT_DIR,\n",
        "    per_device_train_batch_size=BATCH_SIZE,\n",
        "    per_device_eval_batch_size=BATCH_SIZE,\n",
        "    gradient_accumulation_steps=2,  # ë©”ëª¨ë¦¬ê°€ ë¶€ì¡±í•˜ë©´ ì´ ê°’ì„ ë†’ì´ì„¸ìš”\n",
        "    eval_strategy=\"steps\",\n",
        "    eval_steps=EVAL_STEPS,\n",
        "    save_steps=SAVE_STEPS,\n",
        "    save_total_limit=3,  # ìµœëŒ€ 3ê°œì˜ ì²´í¬í¬ì¸íŠ¸ë§Œ ìœ ì§€\n",
        "    num_train_epochs=NUM_EPOCHS,\n",
        "    learning_rate=LEARNING_RATE,\n",
        "    warmup_ratio=0.1,\n",
        "    logging_dir=f\"{OUTPUT_DIR}/logs\",\n",
        "    logging_steps=50,\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"accuracy\",\n",
        "    greater_is_better=True,\n",
        "    push_to_hub=False,\n",
        "    fp16=USE_GPU,  # GPU ì‚¬ìš© ì‹œ mixed precision í™œì„±í™”\n",
        "    dataloader_num_workers=4,\n",
        "    remove_unused_columns=False,\n",
        ")\n",
        "\n",
        "print(\"\\nğŸ“‹ í•™ìŠµ ì„¤ì •:\")\n",
        "print(f\"  â€¢ Batch size: {BATCH_SIZE}\")\n",
        "print(f\"  â€¢ Epochs: {NUM_EPOCHS}\")\n",
        "print(f\"  â€¢ Learning rate: {LEARNING_RATE}\")\n",
        "print(f\"  â€¢ Mixed precision (FP16): {USE_GPU}\")\n",
        "print(f\"  â€¢ Output directory: {OUTPUT_DIR}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 8: Trainer ì´ˆê¸°í™” ë° í•™ìŠµ ì‹œì‘\n",
        "\n",
        "**ì£¼ì˜**: ë‹¤ìŒ ì…€ì€ ì‹¤í–‰ ì‹œê°„ì´ ì˜¤ë˜ ê±¸ë¦½ë‹ˆë‹¤ (GPU ì‚¬ìš© ì‹œ 1-3ì‹œê°„, CPU ì‚¬ìš© ì‹œ ìˆ˜ ì‹œê°„~ìˆ˜ì¼)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Early Stopping ì„¤ì • (validation ì„±ëŠ¥ì´ ê°œì„ ë˜ì§€ ì•Šìœ¼ë©´ ì¡°ê¸° ì¢…ë£Œ)\n",
        "early_stopping = EarlyStoppingCallback(\n",
        "    early_stopping_patience=3,  # 3ë²ˆ ì—°ì† ê°œì„  ì—†ìœ¼ë©´ ì¢…ë£Œ\n",
        "    early_stopping_threshold=0.01\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=encoded_dataset[\"train\"],\n",
        "    eval_dataset=encoded_dataset[\"validation\"],\n",
        "    compute_metrics=compute_metrics,\n",
        "    callbacks=[early_stopping],\n",
        ")\n",
        "\n",
        "print(\"\\nâœ… Trainer ì´ˆê¸°í™” ì™„ë£Œ!\")\n",
        "print(\"\\nğŸš€ í•™ìŠµì„ ì‹œì‘í•©ë‹ˆë‹¤...\\n\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# í•™ìŠµ ì‹œì‘\n",
        "train_result = trainer.train()\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"\\nâœ… í•™ìŠµ ì™„ë£Œ!\\n\")\n",
        "print(\"í•™ìŠµ ê²°ê³¼:\")\n",
        "print(train_result)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 9: ëª¨ë¸ ì €ì¥\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\nğŸ’¾ ìµœì¢… ëª¨ë¸ ì €ì¥ ì¤‘...\")\n",
        "\n",
        "# ìµœì¢… ëª¨ë¸ ì €ì¥\n",
        "final_model_path = f\"{OUTPUT_DIR}/final_model\"\n",
        "trainer.save_model(final_model_path)\n",
        "processor.save_pretrained(final_model_path)\n",
        "\n",
        "print(f\"\\nâœ… ëª¨ë¸ì´ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤: {final_model_path}\")\n",
        "print(\"\\nì €ì¥ëœ íŒŒì¼:\")\n",
        "for file in os.listdir(final_model_path):\n",
        "    print(f\"  â€¢ {file}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 10: í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ í‰ê°€ ë° ê²°ê³¼ ë¶„ì„\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\nğŸ“Š í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ í‰ê°€ ì¤‘...\\n\")\n",
        "\n",
        "# í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ í‰ê°€\n",
        "test_results = trainer.evaluate(encoded_dataset[\"test\"])\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"í…ŒìŠ¤íŠ¸ ê²°ê³¼:\")\n",
        "print(\"=\" * 80)\n",
        "for key, value in test_results.items():\n",
        "    print(f\"  {key}: {value:.4f}\")\n",
        "\n",
        "# í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ ì˜ˆì¸¡\n",
        "predictions = trainer.predict(encoded_dataset[\"test\"])\n",
        "predicted_labels = np.argmax(predictions.predictions, axis=1)\n",
        "true_labels = predictions.label_ids\n",
        "\n",
        "# ìƒì„¸ ë¶„ë¥˜ ë¦¬í¬íŠ¸\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"ìƒì„¸ ë¶„ë¥˜ ë¦¬í¬íŠ¸:\")\n",
        "print(\"=\" * 80)\n",
        "print(classification_report(\n",
        "    true_labels, \n",
        "    predicted_labels, \n",
        "    target_names=label_names,\n",
        "    digits=4\n",
        "))\n",
        "\n",
        "# Confusion Matrix\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"Confusion Matrix:\")\n",
        "print(\"=\" * 80)\n",
        "cm = confusion_matrix(true_labels, predicted_labels)\n",
        "cm_df = pd.DataFrame(cm, index=label_names, columns=label_names)\n",
        "print(cm_df)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 11: ëª¨ë¸ í…ŒìŠ¤íŠ¸ (ì‹¤ì œ ì˜¤ë””ì˜¤ íŒŒì¼ë¡œ ì¶”ë¡ )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import pipeline\n",
        "import tempfile\n",
        "\n",
        "print(\"\\nğŸ§ª ì¶”ë¡  íŒŒì´í”„ë¼ì¸ ìƒì„± ì¤‘...\")\n",
        "\n",
        "# ì¶”ë¡  íŒŒì´í”„ë¼ì¸ ìƒì„±\n",
        "classifier = pipeline(\n",
        "    \"audio-classification\",\n",
        "    model=final_model_path,\n",
        "    device=0 if USE_GPU else -1\n",
        ")\n",
        "\n",
        "print(\"âœ… íŒŒì´í”„ë¼ì¸ ì¤€ë¹„ ì™„ë£Œ!\\n\")\n",
        "\n",
        "# í…ŒìŠ¤íŠ¸ ìƒ˜í”Œë¡œ ì¶”ë¡  (í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ì˜ ì²« 5ê°œ)\n",
        "print(\"=\" * 80)\n",
        "print(\"ìƒ˜í”Œ ì¶”ë¡  í…ŒìŠ¤íŠ¸:\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "test_samples = dataset_split[\"test\"].select(range(min(5, len(dataset_split[\"test\"]))))\n",
        "\n",
        "for idx, sample in enumerate(test_samples):\n",
        "    audio_array = sample[\"audio\"][\"array\"]\n",
        "    true_label = label_names[sample[\"label\"]]\n",
        "    \n",
        "    # ì„ì‹œ íŒŒì¼ë¡œ ì €ì¥ (pipelineì€ íŒŒì¼ ê²½ë¡œ ë˜ëŠ” arrayë¥¼ ë°›ìŒ)\n",
        "    with tempfile.NamedTemporaryFile(suffix=\".wav\", delete=False) as temp_file:\n",
        "        sf.write(temp_file.name, audio_array, SAMPLE_RATE)\n",
        "        temp_path = temp_file.name\n",
        "    \n",
        "    # ì¶”ë¡ \n",
        "    result = classifier(temp_path, top_k=len(label_names))\n",
        "    \n",
        "    # ê²°ê³¼ ì¶œë ¥\n",
        "    print(f\"\\nìƒ˜í”Œ {idx + 1}:\")\n",
        "    print(f\"  ì‹¤ì œ ë ˆì´ë¸”: {true_label}\")\n",
        "    print(f\"  ì˜ˆì¸¡ ê²°ê³¼:\")\n",
        "    for pred in result:\n",
        "        print(f\"    - {pred['label']:15s}: {pred['score']:.4f}\")\n",
        "    \n",
        "    # ì„ì‹œ íŒŒì¼ ì‚­ì œ\n",
        "    os.remove(temp_path)\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ğŸ‰ ì™„ë£Œ! ë‹¤ìŒ ë‹¨ê³„\n",
        "\n",
        "### âœ… ì™„ë£Œëœ ì‘ì—…\n",
        "1. ë°ì´í„°ì…‹ ë¡œë“œ ë° ì´ì§„ ë¶„ë¥˜ ë³€í™˜\n",
        "2. Wav2Vec2 ëª¨ë¸ íŒŒì¸íŠœë‹ (í‘œì¤€ì–´ vs ë¹„í‘œì¤€ì–´)\n",
        "3. ëª¨ë¸ í‰ê°€ ë° ì €ì¥\n",
        "4. ì¶”ë¡  í…ŒìŠ¤íŠ¸\n",
        "\n",
        "### ğŸ“¦ ìƒì„±ëœ íŒŒì¼ (Colab)\n",
        "- ëª¨ë¸ íŒŒì¼: `/content/drive/MyDrive/models/dialect_binary_classifier/final_model/`\n",
        "- ì²´í¬í¬ì¸íŠ¸: `/content/drive/MyDrive/models/dialect_binary_classifier/checkpoint-*/`\n",
        "- ë¡œê·¸: `/content/drive/MyDrive/models/dialect_binary_classifier/logs/`\n",
        "\n",
        "### ğŸ“¥ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ (Colab â†’ ë¡œì»¬)\n",
        "\n",
        "**Google Driveì—ì„œ ëª¨ë¸ í´ë” ë‹¤ìš´ë¡œë“œ:**\n",
        "1. Google Drive ì—´ê¸°\n",
        "2. `MyDrive/models/dialect_binary_classifier/final_model/` ì°¾ê¸°\n",
        "3. í´ë” ì „ì²´ ë‹¤ìš´ë¡œë“œ\n",
        "4. HabitLinkì˜ `models/dialect_binary_classifier/final_model/`ì— ë°°ì¹˜\n",
        "\n",
        "**ë˜ëŠ” Colabì—ì„œ ì§ì ‘ ë‹¤ìš´ë¡œë“œ:**\n",
        "```python\n",
        "# Colabì—ì„œ ì‹¤í–‰\n",
        "from google.colab import files\n",
        "import shutil\n",
        "\n",
        "# ëª¨ë¸ì„ ì••ì¶•\n",
        "shutil.make_archive('dialect_model', 'zip', final_model_path)\n",
        "files.download('dialect_model.zip')\n",
        "```\n",
        "\n",
        "### ğŸš€ HabitLinkì—ì„œ ì‚¬ìš©í•˜ê¸°\n",
        "\n",
        "**1. ëª¨ë¸ ë°°ì¹˜:**\n",
        "```\n",
        "HabitLink/\n",
        "â””â”€â”€ models/\n",
        "    â””â”€â”€ dialect_binary_classifier/\n",
        "        â””â”€â”€ final_model/\n",
        "            â”œâ”€â”€ config.json\n",
        "            â”œâ”€â”€ preprocessor_config.json\n",
        "            â”œâ”€â”€ pytorch_model.bin\n",
        "            â””â”€â”€ ...\n",
        "```\n",
        "\n",
        "**2. ì‹¤í–‰:**\n",
        "```bash\n",
        "python main.py\n",
        "# \"7. ë°©ì–¸ ë¶„ì„\" ì„ íƒ!\n",
        "```\n",
        "\n",
        "**3. ê²°ê³¼ ì˜ˆì‹œ:**\n",
        "```\n",
        "í‘œì¤€ì–´ í™•ë¥ : 85.2%\n",
        "ë¹„í‘œì¤€ì–´ í™•ë¥ : 14.8%\n",
        "â†’ íŒì •: í‘œì¤€ì–´ âœ…\n",
        "```\n",
        "\n",
        "**í”„ë¡œê·¸ë˜ë° ë°©ì‹:**\n",
        "```python\n",
        "from src.dialect_analyzer import DialectAnalyzer\n",
        "\n",
        "analyzer = DialectAnalyzer(\"models/dialect_binary_classifier/final_model\")\n",
        "result = analyzer.analyze(\"audio.wav\")\n",
        "print(result)  # {'standard': 0.85, 'non_standard': 0.15}\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"ğŸ‰ ëª¨ë“  ì‘ì—…ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!\")\n",
        "print(\"=\"*80)\n",
        "print(f\"\\nëª¨ë¸ ê²½ë¡œ: {final_model_path}\")\n",
        "print(\"\\nì´ì œ HabitLink ë©”ì¸ ì• í”Œë¦¬ì¼€ì´ì…˜ì—ì„œ ë°©ì–¸ ë¶„ì„ì„ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤!\")\n",
        "print(\"\\në‹¤ìŒ ëª…ë ¹ì–´ë¡œ í…ŒìŠ¤íŠ¸í•˜ì„¸ìš”:\")\n",
        "print(\"  cd .. && python main.py\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
